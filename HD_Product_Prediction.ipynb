{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains portion of the codes required to solve the Home Depot visual intelligence case study written by Milad Makkie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the images, I realized there are many images with missing contents in the dataset. So I used a perceptual hashing method to find all of those with the same content and to remove them using a simple bash script. All of the low-level perceptual hashing is done through ImageMagick tools. The below script also computes the white percentage of each image and store it in a CSV file which we will later use in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# At first we unzip the dataset.\n",
    "cd /Users/milad/Downloads/home_depot_products\n",
    "for dirs in *\n",
    "do\n",
    "    mkdir /Users/milad/Downloads/HD_files/\"${dirs%.*}\"\n",
    "    unzip $dirs -d /Users/milad/Downloads/HD_files/\"${dirs%.*}\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below script removes ~442 images with no actual content from the data set. It also computes the White Percentage of each one as well as the all of its perceptual hashes. There are four different hashes that can be used: bmh (block mean hash), bdh (block difference hash), avh (annular variance hash) and pfh (polar fft hash)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# DO NOT RUN THIS AT JUPYTER NOTEBOOK! Simply use it as a bash script.\n",
    "counter=0\n",
    "threshold=21\n",
    "cd /home/vagrant/datacourse/HD/HD_files\n",
    "for cats in *\n",
    "do\n",
    "    for item in $cats/*\n",
    "    do\n",
    "        for pic in $item/*\n",
    "        do\n",
    "            value=$(compare -metric ../NoContentImage.jpg ${pic} null: 2>&1)\n",
    "            size=$(stat -c%s $pic)\n",
    "            nullImageSize=$(stat -c%s ../NoContentImage.jpg)\n",
    "            wPer=$(convert ${pic} +repage +level 5% -bordercolor white -border 1 -fill white -fuzz 25% -draw "color 0,0 floodfill" -shave 1 -fuzz 0 -fill black +opaque white -format "%[fx:int(mean*100)]" info:)\n",
    "            \n",
    "            #if the picture size equal to the refrence image or their phash similar then remove the image.\n",
    "            if [ ${value%.*} -lt $threshold -o $size -eq $nullImageSize ]; then\n",
    "                rm $pic\n",
    "                counter=$((counter+1))\n",
    "                echo $counter\n",
    "                continue\n",
    "            fi\n",
    "            #computing the Block Difference Hash (BDH), Annular Variance Hash (AVH) and Polar FFT Hash (PFH) of the image\n",
    "            #and to store them in the comment section of the image metadata. (we later use it to compute the Hamming distance\n",
    "            #of the current image with the image with the highest WP in each item folder.)\n",
    "            /home/vagrant/datacourse/HD/phashes -m all $pic\n",
    "            \n",
    "            # storing the results in a CSV file.\n",
    "            echo ${item%/*}', '${item##*/}', '${pic##*/}', '${wPer} >> /home/vagrant/datacourse/HD/newresults.csv\n",
    "        done\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do further analyses, I found Pandas the most comfortable option.\n",
    "coltypes={'cat':str,'item':str, 'picid':str,'white':str}\n",
    "colnames=['cat','item', 'picid','white']\n",
    "data=pd.read_csv(\"/home/vagrant/datacourse/HD/newresults.csv\", names=colnames, dtype=coltypes, header=None, na_values=[' ']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping rows with missing values if there is any at all. \n",
    "#(A missing value indicates that there have to be some missing components associated with the image so we can ignore the image.)\n",
    "data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>item</th>\n",
       "      <th>picid</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bar_stool</td>\n",
       "      <td>202751872</td>\n",
       "      <td>1734b6b1-e527-4197-888b-594bfa3b283b.jpg</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bar_stool</td>\n",
       "      <td>202751872</td>\n",
       "      <td>1e33de29-391c-4d2e-9ebd-b94b1daf4e39.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bar_stool</td>\n",
       "      <td>202751872</td>\n",
       "      <td>31bb1632-9bcb-4cbc-8709-cacd8147370d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bar_stool</td>\n",
       "      <td>202751872</td>\n",
       "      <td>65b12ffb-6603-4deb-9d86-0a2104844a63.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bar_stool</td>\n",
       "      <td>202751872</td>\n",
       "      <td>67ea78cd-957c-4175-b5ba-bacb17b7473c.jpg</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat        item                                      picid white\n",
       "0  bar_stool   202751872   1734b6b1-e527-4197-888b-594bfa3b283b.jpg    65\n",
       "1  bar_stool   202751872   1e33de29-391c-4d2e-9ebd-b94b1daf4e39.jpg     0\n",
       "2  bar_stool   202751872   31bb1632-9bcb-4cbc-8709-cacd8147370d.jpg     0\n",
       "3  bar_stool   202751872   65b12ffb-6603-4deb-9d86-0a2104844a63.jpg    19\n",
       "4  bar_stool   202751872   67ea78cd-957c-4175-b5ba-bacb17b7473c.jpg    23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat      object\n",
       "item     object\n",
       "picid    object\n",
       "white    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the white pixel perventage to an integer type.\n",
    "data['white']=data['white'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Build a filter that can discard photos that are not useful for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While there could be many approches to solve this section including training an ML model, using histogram of images and so on, I decided to choose a combination of perceptual hash methods and percentage of white pixels (WP).\n",
    "At first WPs were used to to detect images with white background. Then those images with a similar percentage of WP were collected per items category. At the end those images with minimum perceptual hash difference with the image with maximum WP were selected to train our CNN model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating new dataframe to perfom our analysis \n",
    "hammingData=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to execute shell commands through python\n",
    "def execute(cmd):\n",
    "    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "    for stdout_line in iter(popen.stdout.readline, \"\"):\n",
    "        yield stdout_line \n",
    "    popen.stdout.close()\n",
    "    return_code = popen.wait()\n",
    "    if return_code:\n",
    "        raise subprocess.CalledProcessError(return_code, cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to find the similarity of images in an item folder with the image with the highest WP, I compute all of the relative perceptual hashes of images as well as their WP. I used hamming distance for this purpose. Computed hashes include: the Block Difference Hash (BDH), Annular Variance Hash (AVH) and Polar FFT Hash (PFH). The results then added to the corresponding rows of our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "hammingData['relAVH']=0.0\n",
    "hammingData['relBDH']=0.0\n",
    "hammingData['relPFH']=0.0\n",
    "hammingData['relWhite']=0.0\n",
    "grouped=hammingData.groupby(['cat','item'])\n",
    "\n",
    "# Finding the image with the highest WP value and compare the similarity of all other images in the same item folder with that.\n",
    "for name, group in grouped:\n",
    "    maxW=max(group['white'])\n",
    "    maxWpic=group[group['white']==maxW]\n",
    "    if len(maxWpic.index)>1:\n",
    "        wIndex=maxWpic.first_valid_index()\n",
    "        maxWpic=maxWpic.loc[[wIndex]]\n",
    "        \n",
    "    # Computing the Hamming distance of image hashes regarding the maxWpic\n",
    "    if len(group.index)>1:\n",
    "        sourcePath='/home/vagrant/datacourse/HD/HD_files/'+maxWpic.values[0][0].strip()+'/'+maxWpic.values[0][1].strip()+'/'+maxWpic.values[0][2].strip()\n",
    "        for index, row in group.iterrows():\n",
    "            counter+=1\n",
    "            filePath='/home/vagrant/datacourse/HD/HD_files/'+row['cat'].strip()+'/'+row['item'].strip()+'/'+row['picid'].strip()\n",
    "            if filePath==sourcePath:\n",
    "                pass\n",
    "            else:\n",
    "                for stdout in execute([\"/home/vagrant/datacourse/HD/hamming\", '-m','avh', sourcePath, filePath]):\n",
    "                    relAVH=float(stdout.strip('\\n'))\n",
    "                    hammingData.set_value(index,'relAVH',relAVH)\n",
    "                for stdout in execute([\"/home/vagrant/datacourse/HD/hamming\", '-m','bdh', sourcePath, filePath]):\n",
    "                    relBDH=float(stdout.strip('\\n'))\n",
    "                    hammingData.set_value(index,'relBDH',relBDH)\n",
    "                for stdout in execute([\"/home/vagrant/datacourse/HD/hamming\", '-m','pfh', sourcePath, filePath]):\n",
    "                    relPFH=float(stdout.strip('\\n'))\n",
    "                    hammingData.set_value(index,'relPFH',relPFH)\n",
    "                relWhite=maxWpic.values[0][3]-row['white']\n",
    "                hammingData.set_value(index,'relWhite',relWhite)                \n",
    "    #print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next step is to visualize the results of hamming distance per hashing method to choose the best method for filtering the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8b21a865d0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polar FFT HASH is insensitive to all rotations, but not the flip-flop-transpose-transverse attacks.\n",
    "hammingData.hist(column=\"relPFH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8b1f67b050>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b216b44d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block Difference Hash is insensitive to all but geometry (rotation and distortion) attacks. \n",
    "hammingData.hist(column=\"relBDH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8b1f611c90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b216ebf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annular Variance Hash is insenstive to all but the distortion attacks. It is especially insensitive to all rotations and the flip-flop-transpose-transverse attacks.\n",
    "hammingData.hist(column=\"relAVH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8b1f54c210>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAGCtJREFUeJzt3X+QVed93/H3JxBjJCoQwblBQA3tYGX4UTvWhihxo15CpmDL4+WPjGY1Uowd1TuuiS1n6NgQT0dp2p3RtJJrS7aU2UqqcEW1pbIcGLtyTEhuSTxBRMiOESCitUGCNQLZ+uVVXMTib/+4D/Xpele7nLO7V3ufz2tmZ895znPO83xX6H72/Lh3FRGYmVmefq7VEzAzs9ZxCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZcwhYAZI+iNJD5Xcty7p1Bts/xNJ/7b87Mwmj0PAbBhJfybp04X1RZJilLZfGut4EfHRiPj3ab83DAyzqeYQsCxImnkJ3fcB1xXWrwOeHqHtmYh4fgKmZ9YyDgFrW5JOSPq0pO8Ar0n6x5K+LOkFScclfWKUXfcB75F08f+P3wQ+B3QMa9s3bLwtks5KOi3pw4X2ByX9B0mXA48BV0kaTF9XSfo5SVslfVfSDyXtlDR/In8WZqNxCFi7uxG4HpgPfAX4O2ARsA74pKT1I+xzAJgFvDOtXwfsAfqHtRVD4JeAuenYtwBflHRl8aAR8RrwXuD7ETEnfX0f+DiwEfgXwFXAS8AXK9RsNm4OAWt3d0XESWAV8LaI+OOIeD0ivgf8F6Br+A4RcQ54HLgu/UY+N/X/q0LbCuB/F3Y7D/xxRJyPiP8FDAJXj3OOHwU+ExGn0th/BPzOJV7CMivF/8is3Z1M399O8zLMy4VtM2i+sI/k4n2BE8A3U9tfAx9ObScj4tlC/x9GxFBh/R+AOeOc49uBr0j6SaHtAlADBsZ5DLNSHALW7i5+TO5J4HhELB/nfvto/oZ+gp8GxTeB+1LbvhH3Gv98ik4CvxcR3xxhm9mk8uUgy8UB4EfpRvFsSTMkrZL0q6P0/xtgHnAzKQQi4iXghdRWNgTOAL8gaW6h7U+AHklvB5D0NkmdJY9vdkkcApaFiLgAvB94F3Ac+AHN3+rnjtL/NeAg8BbgqcKmvwJ+kZIhEBFPAw8D35P0sqSrgM8Du4FvSPoRsB/4tTLHN7tU8h+VMTPLl88EzMwy5hAwM8vYmCEg6YH0LsinhrV/XNLTkg5L+o+F9m2S+iUdK74RR9I1kg6lbXdJ0sSWYmZml2o8ZwIPAhuKDZLWAp3AOyNiJXBHal9B8803K9M+90iakXa7F/gIsDx9/X/HNDOzqTfm+wQiYp+kpcOa/zVwe3p3IxFxNrV3An2p/bikfmCNpBPAFRGxH0DSl2i+Tf6xscZfsGBBLF06fPjxee2117j88stL7Ttd5Vgz5Fl3jjVDnnWXqfngwYM/iIi3jdWv7JvF3gH8pqQe4P8A/yYi/pbm56bsL/Q7ldrOp+Xh7SOS1A10A9RqNe64445SkxwcHGTOnPG+abM95Fgz5Fl3jjVDnnWXqXnt2rXPjt2rfAjMpPmBXNcCvwrslPRPSh7rZ0REL9AL0NHREfV6vdRxGo0GZfedrnKsGfKsO8eaIc+6J7Pmsk8HnQIejaYDwE+ABTQ/52RJod/i1DaQloe3m5lZC5UNgT8F1gJIegfNd1X+gOa7HrskzZK0jOYN4AMRcRp4VdK16amgDwK7Ks/ezMwqGfNykKSHgTqwIP1ZvNuAB4AH0mOjrwObovnW48OSdgJHgCFgc3q7PsDHaD5pNJvmDeExbwqbmdnkGs/TQTeOsunmUfr3AD0jtD9B8zPdzczsTcLvGDYzy5hDwMwsYw4BM7OMOQTMzDLW1n9e8tDAK3xo69emfNwTt18/5WOamZXhMwEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4yNGQKSHpB0Nv094eHbtkgKSQsKbdsk9Us6Jml9of0aSYfStrvSH5w3M7MWGs+ZwIPAhuGNkpYA/xJ4rtC2AugCVqZ97pE0I22+F/gIsDx9/cwxzcxsao0ZAhGxD3hxhE3/GfgUEIW2TqAvIs5FxHGgH1gjaSFwRUTsj4gAvgRsrDx7MzOrpNQflZHUCQxExN8Nu6qzCNhfWD+V2s6n5eHtox2/G+gGqNVqNBqNMtOkNhu2rB4qtW8VZec7EQYHB1s6fqvkWHeONUOedU9mzZccApIuA/6Q5qWgSRERvUAvQEdHR9Tr9VLHuXvHLu48NPV/PO3ETfUpH/OiRqNB2Z/XdJZj3TnWDHnWPZk1l3mF/KfAMuDiWcBi4ElJa4ABYEmh7+LUNpCWh7ebmVkLXfIjohFxKCJ+MSKWRsRSmpd23h0RzwO7gS5JsyQto3kD+EBEnAZelXRteirog8CuiSvDzMzKGM8jog8DfwNcLemUpFtG6xsRh4GdwBHg68DmiLiQNn8MuI/mzeLvAo9VnLuZmVU05uWgiLhxjO1Lh633AD0j9HsCWHWJ8zMzs0nkdwybmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmlrHx/I3hBySdlfRUoe0/SXpa0nckfUXSvMK2bZL6JR2TtL7Qfo2kQ2nbXekPzpuZWQuN50zgQWDDsLY9wKqI+GfA3wPbACStALqAlWmfeyTNSPvcC3wEWJ6+hh/TzMym2JghEBH7gBeHtX0jIobS6n5gcVruBPoi4lxEHAf6gTWSFgJXRMT+iAjgS8DGiSrCzMzKmTkBx/g94H+k5UU0Q+GiU6ntfFoe3j4iSd1AN0CtVqPRaJSaWG02bFk9NHbHCVZ2vhNhcHCwpeO3So5151gz5Fn3ZNZcKQQkfQYYAnZMzHSaIqIX6AXo6OiIer1e6jh379jFnYcmIucuzYmb6lM+5kWNRoOyP6/pLMe6c6wZ8qx7Mmsu/Qop6UPA+4F16RIPwACwpNBtcWob4KeXjIrtZmbWQqUeEZW0AfgU8IGI+IfCpt1Al6RZkpbRvAF8ICJOA69KujY9FfRBYFfFuZuZWUVjnglIehioAwsknQJuo/k00CxgT3rSc39EfDQiDkvaCRyheZloc0RcSIf6GM0njWYDj6UvMzNroTFDICJuHKH5/jfo3wP0jND+BLDqkmZnZmaTyu8YNjPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjY4aApAcknZX0VKFtvqQ9kp5J368sbNsmqV/SMUnrC+3XSDqUtt2V/uC8mZm10HjOBB4ENgxr2wrsjYjlwN60jqQVQBewMu1zj6QZaZ97gY8Ay9PX8GOamdkUGzMEImIf8OKw5k5ge1reDmwstPdFxLmIOA70A2skLQSuiIj9ERHAlwr7mJlZi8wsuV8tIk6n5eeBWlpeBOwv9DuV2s6n5eHtI5LUDXQD1Go1Go1GuUnOhi2rh0rtW0XZ+U6EwcHBlo7fKjnWnWPNkGfdk1lz2RD4fyIiJMVETKZwzF6gF6CjoyPq9Xqp49y9Yxd3Hqpc4iU7cVN9yse8qNFoUPbnNZ3lWHeONUOedU9mzWWfDjqTLvGQvp9N7QPAkkK/xaltIC0PbzczsxYqGwK7gU1peROwq9DeJWmWpGU0bwAfSJeOXpV0bXoq6IOFfczMrEXGvFYi6WGgDiyQdAq4Dbgd2CnpFuBZ4AaAiDgsaSdwBBgCNkfEhXSoj9F80mg28Fj6MjOzFhozBCLixlE2rRulfw/QM0L7E8CqS5qdmZlNKr9j2MwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMVQoBSX8g6bCkpyQ9LOmtkuZL2iPpmfT9ykL/bZL6JR2TtL769M3MrIrSISBpEfAJoCMiVgEzgC5gK7A3IpYDe9M6klak7SuBDcA9kmZUm76ZmVVR9XLQTGC2pJnAZcD3gU5ge9q+HdiYljuBvog4FxHHgX5gTcXxzcysAkVE+Z2lW4Ee4MfANyLiJkkvR8S8tF3ASxExT9IXgP0R8VDadj/wWEQ8MsJxu4FugFqtdk1fX1+p+Z198RXO/LjUrpWsXjR36gdNBgcHmTNnTsvGb5Uc686xZsiz7jI1r1279mBEdIzVb2bZSaVr/Z3AMuBl4H9KurnYJyJC0iWnTET0Ar0AHR0dUa/XS83x7h27uPNQ6RJLO3FTfcrHvKjRaFD25zWd5Vh3jjVDnnVPZs1VLgf9NnA8Il6IiPPAo8BvAGckLQRI38+m/gPAksL+i1ObmZm1SJUQeA64VtJl6bLPOuAosBvYlPpsAnal5d1Al6RZkpYBy4EDFcY3M7OKSl8riYjHJT0CPAkMAd+ieQlnDrBT0i3As8ANqf9hSTuBI6n/5oi4UHH+ZmZWQaUL5hFxG3DbsOZzNM8KRurfQ/NGspmZvQn4HcNmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZaxSCEiaJ+kRSU9LOirp1yXNl7RH0jPp+5WF/tsk9Us6Jml99embmVkVVc8EPg98PSJ+GXgncBTYCuyNiOXA3rSOpBVAF7AS2ADcI2lGxfHNzKyC0iEgaS5wHXA/QES8HhEvA53A9tRtO7AxLXcCfRFxLiKOA/3AmrLjm5lZdYqIcjtK7wJ6gSM0zwIOArcCAxExL/UR8FJEzJP0BWB/RDyUtt0PPBYRj4xw7G6gG6BWq13T19dXao5nX3yFMz8utWslqxfNnfpBk8HBQebMmdOy8Vslx7pzrBnyrLtMzWvXrj0YER1j9ZtZelbNfd8NfDwiHpf0edKln4siIiRdcspERC/NgKGjoyPq9XqpCd69Yxd3HqpSYjknbqpP+ZgXNRoNyv68prMc686xZsiz7smsuco9gVPAqYh4PK0/QjMUzkhaCJC+n03bB4Alhf0XpzYzM2uR0iEQEc8DJyVdnZrW0bw0tBvYlNo2AbvS8m6gS9IsScuA5cCBsuObmVl1Va+VfBzYIektwPeAD9MMlp2SbgGeBW4AiIjDknbSDIohYHNEXKg4vpmZVVApBCLi28BINx7WjdK/B+ipMqaZmU0cv2PYzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4xVDgFJMyR9S9JX0/p8SXskPZO+X1nou01Sv6RjktZXHdvMzKqZiDOBW4GjhfWtwN6IWA7sTetIWgF0ASuBDcA9kmZMwPhmZlZSpRCQtBi4Hriv0NwJbE/L24GNhfa+iDgXEceBfmBNlfHNzKyaqmcCnwM+Bfyk0FaLiNNp+XmglpYXAScL/U6lNjMza5GZZXeU9H7gbEQclFQfqU9EhKQocexuoBugVqvRaDRKzbE2G7asHiq1bxVl5zsRBgcHWzp+q+RYd441Q551T2bNpUMAeA/wAUnvA94KXCHpIeCMpIURcVrSQuBs6j8ALCnsvzi1/YyI6AV6ATo6OqJer5ea4N07dnHnoSollnPipvqUj3lRo9Gg7M9rOsux7hxrhjzrnsyaS18OiohtEbE4IpbSvOH7FxFxM7Ab2JS6bQJ2peXdQJekWZKWAcuBA6VnbmZmlU3Gr8m3Azsl3QI8C9wAEBGHJe0EjgBDwOaIuDAJ45uZ2ThNSAhERANopOUfAutG6dcD9EzEmGZmVp3fMWxmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxqb+7bQZWLr1ay0b+8ENl7dsbDObfnwmYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnGHAJmZhnzZwfZhGjl5yWduP36lo1tNt2VPhOQtETSX0o6IumwpFtT+3xJeyQ9k75fWdhnm6R+ScckrZ+IAszMrLwqZwJDwJaIeFLSPwIOStoDfAjYGxG3S9oKbAU+LWkF0AWsBK4C/lzSOyLiQrUSrOjQwCt8qIW/lZvZ9FL6TCAiTkfEk2n5R8BRYBHQCWxP3bYDG9NyJ9AXEeci4jjQD6wpO76ZmVWniKh+EGkpsA9YBTwXEfNSu4CXImKepC8A+yPiobTtfuCxiHhkhON1A90AtVrtmr6+vlLzOvviK5z5caldp63abLKrefWiuQwODjJnzpxWT2VK5Vgz5Fl3mZrXrl17MCI6xupX+cawpDnAl4FPRsSrzdf9pogISZecMhHRC/QCdHR0RL1eLzW3u3fs4s5Ded373rJ6KLuaT9xUp9FoUPbfyXSVY82QZ92TWXOlVwtJP08zAHZExKOp+YykhRFxWtJC4GxqHwCWFHZfnNrMpiU/EWXtoHQIpEs99wNHI+KzhU27gU3A7en7rkL7f5f0WZo3hpcDB8qOb3bR0q1fY8vqId8QNyuhypnAe4DfBQ5J+nZq+0OaL/47Jd0CPAvcABARhyXtBI7QfLJos58MMjNrrdIhEBF/DWiUzetG2acH6Ck7ppmZTay87iCatYlWXQLzvYj2488OMjPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjfkTUzN70ih/RMdWPxrb7Y7E+EzAzy5jPBMxs3Fr5oXk2OXwmYGaWMYeAmVnGHAJmZhlzCJiZZcwhYGaWMYeAmVnG/IiomdkbaNVjsVP1JjWfCZiZZWzKQ0DSBknHJPVL2jrV45uZ2U9NaQhImgF8EXgvsAK4UdKKqZyDmZn91FSfCawB+iPiexHxOtAHdE7xHMzMLFFETN1g0u8AGyLiX6X13wV+LSJ+f1i/bqA7rV4NHCs55ALgByX3na5yrBnyrDvHmiHPusvU/PaIeNtYnd6UTwdFRC/QW/U4kp6IiI4JmNK0kWPNkGfdOdYMedY9mTVP9eWgAWBJYX1xajMzsxaY6hD4W2C5pGWS3gJ0AbuneA5mZpZM6eWgiBiS9PvAnwEzgAci4vAkDln5ktI0lGPNkGfdOdYMedY9aTVP6Y1hMzN7c/E7hs3MMuYQMDPLWFuGQC4fTSFpiaS/lHRE0mFJt6b2+ZL2SHomfb+y1XOdaJJmSPqWpK+m9RxqnifpEUlPSzoq6dfbvW5Jf5D+bT8l6WFJb23HmiU9IOmspKcKbaPWKWlben07Jml9lbHbLgQy+2iKIWBLRKwArgU2p1q3AnsjYjmwN623m1uBo4X1HGr+PPD1iPhl4J0062/buiUtAj4BdETEKpoPk3TRnjU/CGwY1jZinen/8S5gZdrnnvS6V0rbhQAZfTRFRJyOiCfT8o9ovigsolnv9tRtO7CxNTOcHJIWA9cD9xWa273mucB1wP0AEfF6RLxMm9dN8wnG2ZJmApcB36cNa46IfcCLw5pHq7MT6IuIcxFxHOin+bpXSjuGwCLgZGH9VGpra5KWAr8CPA7UIuJ02vQ8UGvRtCbL54BPAT8ptLV7zcuAF4D/mi6D3Sfpctq47ogYAO4AngNOA69ExDdo45qHGa3OCX2Na8cQyI6kOcCXgU9GxKvFbdF8BrhtngOW9H7gbEQcHK1Pu9WczATeDdwbEb8CvMawyyDtVne6Bt5JMwCvAi6XdHOxT7vVPJrJrLMdQyCrj6aQ9PM0A2BHRDyams9IWpi2LwTOtmp+k+A9wAcknaB5qe+3JD1Ee9cMzd/2TkXE42n9EZqh0M51/zZwPCJeiIjzwKPAb9DeNReNVueEvsa1Ywhk89EUkkTzGvHRiPhsYdNuYFNa3gTsmuq5TZaI2BYRiyNiKc3/tn8RETfTxjUDRMTzwElJV6emdcAR2rvu54BrJV2W/q2vo3nfq51rLhqtzt1Al6RZkpYBy4EDpUeJiLb7At4H/D3wXeAzrZ7PJNb5z2meIn4H+Hb6eh/wCzSfJngG+HNgfqvnOkn114GvpuW2rxl4F/BE+u/9p8CV7V438O+Ap4GngP8GzGrHmoGHad73OE/zrO+WN6oT+Ex6fTsGvLfK2P7YCDOzjLXj5SAzMxsnh4CZWcYcAmZmGXMImJllzCFgZpYxh4CZWcYcAmZmGfu/W1TcemLbekAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b1f4d5690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relative White shows the diference between the image with maximum WP compare to the rest of the images in an item folder\n",
    "hammingData.hist(column=\"relWhite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>white</th>\n",
       "      <th>relAVH</th>\n",
       "      <th>relBDH</th>\n",
       "      <th>relPFH</th>\n",
       "      <th>relWhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3016.000000</td>\n",
       "      <td>3016.000000</td>\n",
       "      <td>3016.000000</td>\n",
       "      <td>3016.000000</td>\n",
       "      <td>3016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>59.496353</td>\n",
       "      <td>30.222381</td>\n",
       "      <td>26.651525</td>\n",
       "      <td>28.455305</td>\n",
       "      <td>21.785477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27.953230</td>\n",
       "      <td>21.669686</td>\n",
       "      <td>20.648219</td>\n",
       "      <td>23.396756</td>\n",
       "      <td>26.391718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.800000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>47.800000</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>48.200000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.700000</td>\n",
       "      <td>71.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             white       relAVH       relBDH       relPFH     relWhite\n",
       "count  3016.000000  3016.000000  3016.000000  3016.000000  3016.000000\n",
       "mean     59.496353    30.222381    26.651525    28.455305    21.785477\n",
       "std      27.953230    21.669686    20.648219    23.396756    26.391718\n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "25%      37.000000     0.000000     0.000000     0.000000     0.000000\n",
       "50%      65.000000    39.600000    32.000000    34.800000     9.000000\n",
       "75%      85.000000    47.800000    44.500000    48.200000    38.000000\n",
       "max     100.000000    66.700000    71.900000    92.000000    99.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a sense of our numerical data in our dataframe\n",
    "hammingData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BDH_bar_stool_152 BDH_White_bar_stool_172 PFH_bar_stool_106\n",
      "BDH_bookcase_193 BDH_White_bookcase_218 PFH_bookcase_105\n",
      "BDH_chandelier_225 BDH_White_chandelier_289 PFH_chandelier_112\n",
      "BDH_dining_chair_155 BDH_White_dining_chair_257 PFH_dining_chair_140\n",
      "BDH_market_umbrella_114 BDH_White_market_umbrella_175 PFH_market_umbrella_102\n",
      "BDH_night_stands_14 BDH_White_night_stands_19 PFH_night_stands_10\n",
      "BDH_ottoman_112 BDH_White_ottoman_126 PFH_ottoman_100\n",
      "BDH_sconces_191 BDH_White_sconces_275 PFH_sconces_153\n",
      "BDH_table_lamp_141 BDH_White_table_lamp_173 PFH_table_lamp_121\n",
      "BDH_vases_115 BDH_White_vases_120 PFH_vases_108\n",
      "Total BDH:1412\n",
      "Total PFH:1057\n",
      "Total BDH_White:1824\n"
     ]
    }
   ],
   "source": [
    "# Using the above data multiple comparisons were performed, and the results showed relBDH combined with relWhite will \n",
    "# filter the minimum number of images.\n",
    "grouped=hammingData.groupby(['cat'])\n",
    "counter_BDH_total=0\n",
    "counter_PFH_total=0\n",
    "counter_BDH_White_total=0\n",
    "for name,group in grouped:\n",
    "    counter_BDH=0\n",
    "    counter_PFH=0\n",
    "    counter_BDH_White=0\n",
    "    for index, row in group.iterrows():\n",
    "        if row['relBDH']<30:            \n",
    "            counter_BDH+=1\n",
    "            counter_BDH_total+=1\n",
    "        if row['relPFH']<=10:\n",
    "            counter_PFH+=1\n",
    "            counter_PFH_total+=1\n",
    "        if row['relPFH']<=30 or row['relWhite']<10:\n",
    "            counter_BDH_White+=1\n",
    "            counter_BDH_White_total+=1\n",
    "        \n",
    "    print('BDH_'+name+'_'+str(counter_BDH), 'BDH_White_'+name+'_'+str(counter_BDH_White),  'PFH_'+name+'_'+str(counter_PFH))\n",
    "print('Total BDH:'+str(counter_BDH_total))\n",
    "print('Total PFH:'+str(counter_PFH_total))\n",
    "print('Total BDH_White:'+str(counter_BDH_White_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After evaluating the results of all relative hash scores, BDH was the perfomer in terms of filtering images with non-white background. (possible images with lifestyle, infomational or details groups that are not good for training on our CNN model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally I choose the relative Block Difference Hash score which is sensitive to geometry of images to filter the pictures.\n",
    "# The relative percentage of white pixels were also used as another parameter for filtering the data. \n",
    "# Thresholds optimization was performed manually.\n",
    "grouped=hammingData.groupby(['cat'])\n",
    "for name,group in grouped:\n",
    "    counter=0\n",
    "    destPath='/home/vagrant/datacourse/HD/hammingData_BDH_White/'+name\n",
    "    destPath_test='/home/vagrant/datacourse/HD/hammingData_BDH_White_test/'+name\n",
    "    if not os.path.exists(destPath):\n",
    "        os.makedirs(destPath)\n",
    "    if not os.path.exists(destPath_test):\n",
    "        os.makedirs(destPath_test)\n",
    "    for index, row in group.iterrows():\n",
    "        filePath='/home/vagrant/datacourse/HD/HD_files/'+row['cat'].strip()+'/'+row['item'].strip()+'/'+row['picid'].strip()            \n",
    "        if row['relBDH']<=30 or row['relWhite']<=10:\n",
    "            shutil.copy(filePath,destPath)\n",
    "            counter+=1\n",
    "        else:\n",
    "            shutil.copy(filePath,destPath_test)\n",
    "            \n",
    "    print(name)\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1823 images were remained out of 3014 images which can be used for training our CNN model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tBuild a classifier that can automatically classify the category of images in one of the 10 categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given our very small dataset of images (only ~1800 images with some categories with only 16 examples), there is no point on training a model from scratch, therefore I decided to reuse the feature extraction capabilities from one of the powerful image classifiers trained on ImageNet and just train a new classification layer on top. \n",
    "\n",
    "* After multiple comparisons between the winners of the ImageNet competitions since 2012, I chose InceptionV3 architecture given its overall performance and its complexity.\n",
    "\n",
    "* Tensorflow Hub was used for the rest of this notebook. The split sizes are %80 training, &10 Validation and %10 Testing to predict the real performance of the classifier.\n",
    "\n",
    "* The first phase analyzes all the images of our dataset and calculates and caches the bottleneck values for each of them. 'Bottleneck' refers to the layer just before the final output layer that does the classification. (TensorFlow Hub calls this an \"image feature vector.\") This penultimate layer has been trained to output a set of values that's good enough for the classifier to use to distinguish between all the classes it's been asked to recognize.\n",
    "* The next is to train our model based on our data. The training's objective is to make the loss as small as possible, so you can tell if the learning is working by keeping an eye on whether the loss keeps trending downwards, ignoring the short-term noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A complete script to create the last layer on top of out pre_trained model and to train our data accordingly.\n",
    "# This script is available at https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py\n",
    "# --random_crop and --random_brightness are used to add distortion to the images. \n",
    "# --flip_left_right was also used to mirror half of the images horizontally randomly. \n",
    "\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=4000 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/Inception_v3/complete \\\n",
    "  --output_graph=tf_files/output_graph/Inception_v3/complete/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --print_misclassified_test_images \\\n",
    "  --random_crop=30 \\\n",
    "  --random_brightness=30 \\\n",
    "  --flip_left_right \\\n",
    "  --image_dir=/home/vagrant/datacourse/HD/hammingData_BDH_White"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each step chooses ten images at random from the training set, finds their bottlenecks from the cache, and feeds them into the final layer to get predictions. Those predictions are then compared against the actual labels to update the final layer's weights through the back-propagation process.\n",
    "\n",
    "* The results are visualized using Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To evaluate our trained model performance, a never seen image can be used.\n",
    "# An example of the code is shown as below\n",
    "python scripts/label_image.py \\\n",
    "--graph=tf_files/final_results_simple/output_graph/Inception_v3/retrained_graph.pb \\\n",
    "--labels=tf_files/final_results_simple/retrained_labels.txt  \\\n",
    "--input_layer=Placeholder \\\n",
    "--output_layer=final_result \\\n",
    "--image=/home/vagrant/datacourse/HD/hammingData_BDH_White/bar_stool/7342b5cf-126a-4803-9f78-da13b5ddda9d.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final result using 4000 training steps shows that the validation accuracy can be as high as %98 (N=100) and the test accuracy is %89.7 (N=165)\n",
    "\n",
    "* The result indicates that the validation accuracy fluctuates among iterations. Much of this fluctuation arises from the fact that a random subset of the validation set is chosen for each validation accuracy measurement. This can be reduced with the cost of increasing in training time by let say setting the validation batch size to use entire dataset. (not recommended)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
